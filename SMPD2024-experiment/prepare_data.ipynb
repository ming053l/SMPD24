{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ming0531/miniconda3/envs/drct/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\")\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"train\"\n",
    "# temporal\n",
    "with open(\"/ssd5/ming/CPDN/dataset/train_allmetadata_json/train_temporalspatial_information.json\" % (data_type, data_type)) as f:\n",
    "    train_ts = json.load(f)\n",
    "    vuid_arr = []\n",
    "    time_arr = []\n",
    "    lon_arr, lat_arr, geoacc_arr = [], [], []\n",
    "    ts_dict = {}\n",
    "    # 对时间排序\n",
    "    for k, v in enumerate(train_ts):\n",
    "        vuid = v['Uid'] + '/' + v['Pid']\n",
    "        vuid_arr.append(vuid)\n",
    "        time_arr.append(v['Postdate'])\n",
    "        if v['Geoaccuracy'] == '0':\n",
    "            lon_arr.append(0)\n",
    "            lat_arr.append(0)\n",
    "            geoacc_arr.append(0)\n",
    "        else:\n",
    "            lon_arr.append(v['Longitude'])\n",
    "            lat_arr.append(v['Latitude'])\n",
    "            geoacc_arr.append(v['Geoaccuracy'])\n",
    "    id_arr = list(range(0, len(time_arr)))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"/ssd5/ming/CPDN/dataset/%s_allmetadata_json/%s_category.json\" % (data_type, data_type)) as f:\n",
    "    train_cat = json.load(f)\n",
    "    cat_arr, subcat_arr, concept_arr = [], [], []\n",
    "    cat_sort, subcat_sort, concept_sort = [], [], []\n",
    "    for v in train_cat:\n",
    "        cat_arr.append(v['Category'])\n",
    "        subcat_arr.append(v['Subcategory'])\n",
    "        concept_arr.append(v['Concept'])\n",
    "    for id in id_sort:\n",
    "        cat_sort.append(cat_arr[id])\n",
    "        subcat_sort.append(subcat_arr[id])\n",
    "        concept_sort.append(concept_arr[id])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"/ssd5/ming/CPDN/dataset/%s_allmetadata_json/%s_text.json\" % (data_type, data_type)) as f:\n",
    "    train_text = json.load(f)\n",
    "    text_arr, text_sort = [], []\n",
    "    tags_arr, tags_sort = [], []\n",
    "    for v in train_text:\n",
    "        text_arr.append(v['Title'])\n",
    "        tags_arr.append(v['Alltags'])\n",
    "    for id in id_sort:\n",
    "        text_sort.append(text_arr[id])\n",
    "        tags_sort.append(tags_arr[id])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "486194"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(path_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tensor([     0,    139, 218646, 438926, 444739, 444740, 448089, 452556])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'architect4'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_arr[438926]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'luisdrayton'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in path_arr:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type='test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/ssd5/ming/CPDN/dataset/%s_allmetadata_json/%s_additional_information.json\" % (data_type, data_type)) as f:\n",
    "    train_add = json.load(f)\n",
    "    #public_arr, public_sort, path_arr, path_sort = [], [], [], []\n",
    "    for v in train_add:\n",
    "        vuid = v['Uid'] + '/' + v['Pid']\n",
    "        public_arr.append(v['Ispublic'])\n",
    "        path_arr.append(v['Pathalias'])\n",
    "    for id in id_sort:\n",
    "        public_sort.append(public_arr[id])\n",
    "        path_sort.append(path_arr[id])\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_type == \"train\":\n",
    "    with open(\"/ssd5/ming/CPDN/dataset/train_allmetadata_json/train_label.txt\") as f:\n",
    "        label_arr, label_sort = [], []\n",
    "        for ls in f.readlines():\n",
    "            l = ls.strip().split(' ')[0]\n",
    "            label_arr.append(float(l))\n",
    "        for id in id_sort:\n",
    "            label_sort.append(label_arr[id])\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user\n",
    "fill_loc = \"0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\"\n",
    "with open(\"/ssd5/ming/CPDN/dataset/%s_allmetadata_json/%s_user_data.json\" % (data_type, data_type)) as f:\n",
    "    train_user = json.load(f)\n",
    "    ispro_arr, ispro_sort, pcount_arr, pcount_sort, canpro_arr, canpro_sort = [], [], [], [], [], []\n",
    "    loc_arr, loc_sort = [], []\n",
    "    tzid_arr, tzid_sort, tzoffset_arr, tzoffset_sort = [], [], [], []\n",
    "    loc_str = ''\n",
    "    for v in train_user:\n",
    "        ispro_arr.append(v['ispro'])\n",
    "        pcount_arr.append(v['photo_count'])\n",
    "        canpro_arr.append(v['canbuypro'])\n",
    "        tzid_arr.append(v['timezone_timezone_id'])\n",
    "        tzoffset_arr.append(v['timezone_offset'])\n",
    "        if v['location_description'] == \"\\n\":\n",
    "            loc_str = fill_loc\n",
    "        else:\n",
    "            loc_str = v['location_description']\n",
    "        loc_str_list = loc_str.split(',')\n",
    "        loc_list = []\n",
    "        for i in loc_str_list:\n",
    "            loc_list.append(float(i))\n",
    "        loc_arr.append(loc_list)\n",
    "    #print(vuid_arr[165118], vuid_arr[123131])\n",
    "    #print(pcount_arr[165118], pcount_arr[123131])\n",
    "    #print(ispro_arr[165118], ispro_arr[123131])\n",
    "    for id in id_sort:\n",
    "        ispro_sort.append(ispro_arr[id])\n",
    "        pcount_sort.append(pcount_arr[id])\n",
    "        canpro_sort.append(canpro_arr[id])\n",
    "        tzid_sort.append(tzid_arr[id])\n",
    "        tzoffset_sort.append(tzoffset_arr[id])\n",
    "        loc_sort.append((loc_arr[id]))\n",
    "pcount_sort = list(preprocessing.minmax_scale(pcount_sort))\n",
    "f.close()\n",
    "np.save('/ssd5/ming/CPDN/dataset/features/test_location.npy', np.asarray(loc_sort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_type == \"train\":\n",
    "    data = zip(*[id_sort, vuid_sort, time_sort, cat_sort, subcat_sort, concept_sort, text_sort, tags_sort, public_sort, ispro_sort, pcount_sort , canpro_sort, tzid_sort\\\n",
    "        ,tzoffset_sort, public_sort, path_sort, lon_sort, lat_sort, geoacc_sort,  label_sort])\n",
    "    dataframe = pd.DataFrame(data, columns=['id', 'vuid', 'time', 'category', 'subcategory', 'concept', 'text', 'tags', 'ispublic', 'ispro', 'pcount', 'canbuypro',\\\n",
    "        'tzid','tzoffset_sort', 'public', 'pathalias', 'longitude', 'latitude', 'geoacc', 'label'])\n",
    "    dataframe.to_csv('/ssd5/ming/CPDN/dataset/train_data_0.csv')\n",
    "\n",
    "else:\n",
    "    data = zip(*[id_sort, vuid_sort, time_sort, cat_sort, subcat_sort, concept_sort, text_sort, tags_sort, public_sort, ispro_sort, pcount_sort , canpro_sort, tzid_sort\\\n",
    "        ,tzoffset_sort, public_sort, path_sort,  lon_sort, lat_sort, geoacc_sort])\n",
    "    dataframe = pd.DataFrame(data, columns=['id', 'vuid', 'time', 'category', 'subcategory', 'concept', 'text', 'tags', 'ispublic', 'ispro', 'pcount', 'canbuypro',\\\n",
    "        'tzid','tzoffset_sort', 'public', 'pathalias', 'longitude', 'latitude', 'geoacc'])\n",
    "    dataframe.to_csv('/ssd5/ming/CPDN/dataset/test_data_0.csv')\n",
    "\n",
    "\n",
    "if data_type == \"train\":\n",
    "    data = zip(*[id_arr, vuid_arr, time_arr, cat_arr, subcat_arr, concept_arr, text_arr, tags_arr, public_arr, ispro_arr, pcount_arr , canpro_arr, tzid_arr\\\n",
    "        ,tzoffset_arr, public_arr, path_arr, lon_arr, lat_arr, geoacc_arr, label_arr])\n",
    "    dataframe = pd.DataFrame(data, columns=['id', 'vuid', 'time', 'category', 'subcategory', 'concept', 'text', 'tags', 'ispublic', 'ispro', 'pcount', 'canbuypro',\\\n",
    "        'tzid','tzoffset_sort', 'public', 'pathalias','longitude', 'latitude', 'geoacc', 'label'])\n",
    "    dataframe.to_csv('/ssd5/ming/CPDN/dataset/train_data_1.csv')\n",
    "else:\n",
    "    data = zip(*[id_arr, vuid_arr, time_arr, cat_arr, subcat_arr, concept_arr, text_arr, tags_arr, public_arr, ispro_arr, pcount_arr , canpro_arr, tzid_arr\\\n",
    "        ,tzoffset_arr, public_arr, path_arr, lon_arr, lat_arr, geoacc_arr])\n",
    "    dataframe = pd.DataFrame(data, columns=['id', 'vuid', 'time', 'category', 'subcategory', 'concept', 'text', 'tags', 'ispublic', 'ispro', 'pcount', 'canbuypro',\\\n",
    "        'tzid','tzoffset_sort', 'public', 'pathalias', 'longitude', 'latitude', 'geoacc'])\n",
    "    dataframe.to_csv('/ssd5/ming/CPDN/dataset/test_data_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = []\n",
    "text_features = []\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.size())\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596 461\n"
     ]
    }
   ],
   "source": [
    "batch = 512\n",
    "null=0\n",
    "a = int(len(vuid_arr) / batch)\n",
    "b = len(vuid_arr) - a * batch\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180581"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "352*512+357"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a social media post, the category of this post isFashion, the subcategory is Fashion, the concept is glam, the title is Luis Drayton - Edinburgh shoot #6, and the hashtags arerock punk transgender tranny electronicmusic electro glam electronica luisdrayton fusionrecords thefusionnetwork lmwcphotography.\n"
     ]
    }
   ],
   "source": [
    "sentence = str(\"This is a social media post, the category of this post is\" + cat_arr[0] + \", the subcategory is \" + subcat_arr[0]\\\n",
    "         + \", the concept is \" + concept_arr[0] + \", the title is \" + text_arr[0] + \", and the hashtags are\" + tags_arr[0] +\".\")\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vuid_arr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[1;32m      2\u001b[0m null\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mvuid_arr\u001b[49m) \u001b[38;5;241m/\u001b[39m batch)\n\u001b[1;32m      4\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(vuid_arr) \u001b[38;5;241m-\u001b[39m a \u001b[38;5;241m*\u001b[39m batch\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, a):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#print(j)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vuid_arr' is not defined"
     ]
    }
   ],
   "source": [
    "batch = 256\n",
    "null=0\n",
    "a = int(len(vuid_arr) / batch)\n",
    "b = len(vuid_arr) - a * batch\n",
    "for j in range(0, a):\n",
    "    #print(j)\n",
    "    images = []\n",
    "    texts = []\n",
    "    for i in range(j * batch, (j + 1) * batch):\n",
    "        #text = ' '.join(tags_arr[i])\n",
    "        text = str(\"This is a social media post, the category of this post is\" + cat_arr[0] + \", the subcategory is \" + subcat_arr[0]\\\n",
    "         + \", the concept is \" + concept_arr[0] + \", the title is \" + text_arr[0] + \", and the hashtags are\" + tags_arr[0] +\".\")\n",
    "        image_path = \"/ssd6/ming/eva_embedding/train/\" + vuid_arr[i] + '.jpg'\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "        except:\n",
    "            print(null,image_path)\n",
    "            null=null+1\n",
    "            image = Image.open('/ssd5/ming/CPDN/blank.png')\n",
    "            #print(null,image_path)\n",
    "        images.append(image)\n",
    "        texts.append(text)\n",
    "    inputs = processor(text=texts, images=images, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    image_feature = outputs.vision_model_output.pooler_output\n",
    "    text_feature = outputs.text_model_output.pooler_output\n",
    "    print(image_feature.shape, text_feature.shape)\n",
    "    image_features.append(image_feature.cpu().numpy())\n",
    "    text_features.append(text_feature.cpu().numpy())\n",
    "#82m54.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "texts = []\n",
    "for i in range(a * batch, a * batch + b):\n",
    "    print(i)\n",
    "    text = str(\"This is a social media post, the category of this post is\" + cat_arr[i] + \", the subcategory is \" + subcat_arr[i]\\\n",
    "         + \", the concept is \" + concept_arr[i] + \", the title is \" + text_arr[i] + \", and the hashtags are\" + tags_arr[i] +\".\")\n",
    "\n",
    "    image_path = \"/ssd6/ming/eva_embedding/test/\" + vuid_arr[i] + '.jpg'\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except:\n",
    "        null=null+1\n",
    "        image = Image.open('/ssd5/ming/CPDN/blank.png')\n",
    "        print(null)\n",
    "    images.append(image)\n",
    "    texts.append(text)\n",
    "inputs = processor(text=texts, images=images, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "image_feature = outputs.vision_model_output.pooler_output\n",
    "text_feature = outputs.text_model_output.pooler_output\n",
    "image_features.append(image_feature.cpu().numpy())\n",
    "text_features.append(text_feature.cpu().numpy())\n",
    "#5.7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305613"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*512+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.vstack(text_features)\n",
    "np.save(\"/ssd5/ming/CPDN/dataset/features/test_text_clip.npy\", n)\n",
    "m = np.vstack(image_features)\n",
    "np.save(\"/ssd5/ming/CPDN/dataset/features/test_image_clip.npy\", m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/ssd5/ming/CPDN/dataset/%s_allmetadata_json/%s_additional_information.json\" % (data_type, data_type)) as f:\n",
    "    train_add = json.load(f)\n",
    "    public_arr, public_sort, path_arr, path_sort = [], [], [], []\n",
    "    for v in train_add:\n",
    "        vuid = v['Uid'] + '/' + v['Pid']\n",
    "        public_arr.append(v['Ispublic'])\n",
    "        path_arr.append(v['Pathalias'])\n",
    "    for id in id_sort:\n",
    "        public_sort.append(public_arr[id])\n",
    "        path_sort.append(path_arr[id])\n",
    "f.close()\n",
    "additional_data = pd.read_csv(\"/ssd5/ming/CPDN/dataset/clean_user_additional_20240424.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'testimonials_nums', 'photosof_nums', 'photoCount',\n",
       "       'person_models_length', 'totalViews', 'totalTags', 'totalGeotagged',\n",
       "       'totalFaves', 'totalInGroup', 'followerCount', 'followingCount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info_dict = additional_data.set_index('name').T.to_dict('list')\n",
    "\n",
    "# Creating a result dictionary that will contain the usernames from path_arr\n",
    "# and their corresponding additional information if found in the DataFrame\n",
    "additional_dict = {username: user_info_dict.get(username) for username in path_arr}\n",
    "additional_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/ssd5/ming/CPDN/dataset/test_additional.json\", \"w\") as f:\n",
    "    json.dump(additional_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testimonials_nums, photosof_nums, photoCount, person_models_length, totalViews, totalTags = [], [], [], [], [], []\n",
    "totalGeotagged, totalFaves, totalInGroup, followerCount, followingCount = [], [], [], [], []\n",
    "for id in path_arr:\n",
    "    try:\n",
    "        testimonials_nums.append(additional_dict[id][0])\n",
    "    except:\n",
    "        testimonials_nums.append(0)\n",
    "for id in path_arr:\n",
    "    try:\n",
    "        photosof_nums.append(additional_dict[id][1])\n",
    "    except:\n",
    "        photosof_nums.append(0)\n",
    "for id in path_arr:\n",
    "    try:\n",
    "        photoCount.append(additional_dict[id][2])\n",
    "    except:\n",
    "        photoCount.append(0)\n",
    "for id in path_arr:\n",
    "    try:\n",
    "        person_models_length.append(additional_dict[id][3])\n",
    "    except:\n",
    "        person_models_length.append(0)\n",
    "\n",
    "for id in path_arr:\n",
    "    try:\n",
    "        totalViews.append(additional_dict[id][4])\n",
    "    except:\n",
    "        totalViews.append(0)\n",
    "for id in path_arr:\n",
    "    try:\n",
    "        totalTags.append(additional_dict[id][5])\n",
    "    except:\n",
    "        totalTags.append(0)\n",
    "for id in path_arr:\n",
    "    try:\n",
    "        totalGeotagged.append(additional_dict[id][6])\n",
    "    except:\n",
    "        totalGeotagged.append(0)\n",
    "for id in path_arr:\n",
    "    try:\n",
    "        totalFaves.append(additional_dict[id][7])\n",
    "    except:\n",
    "        totalFaves.append(0)  \n",
    "for id in path_arr:\n",
    "    try:\n",
    "        totalInGroup.append(additional_dict[id][8])\n",
    "    except:\n",
    "        totalInGroup.append(0)\n",
    "for id in path_arr:\n",
    "    try:\n",
    "        followerCount.append(additional_dict[id][9])\n",
    "    except:\n",
    "        followerCount.append(0)\n",
    "        \n",
    "for id in path_arr:\n",
    "    try:\n",
    "        followingCount.append(additional_dict[id][10])\n",
    "    except:\n",
    "        followingCount.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type='test'\n",
    "with open(\"/ssd5/ming/CPDN/dataset2/%s_allmetadata_json/%s_user_data.json\" % (data_type, data_type)) as f:\n",
    "    train_user = json.load(f)\n",
    "pattern = r'\\b(?:http://|www\\.)\\S+?\\b'\n",
    "\n",
    "phone_pattern = r'\\(?\\d{1,3}\\)?[\\s-]?\\d{1,4}[\\s-]?\\d{1,4}[\\s-]?\\d{1,4}'\n",
    "\n",
    "# URL模式 - 匹配典型的网址\n",
    "url_pattern = r'\\b(?:https?://)?\\w+\\.com\\b(?:[\\/\\w-]*)*'\n",
    "\n",
    "# 将电话号码和URL的正则表达式合并\n",
    "combined_pattern = f'{phone_pattern}|{url_pattern}'\n",
    "\n",
    "#\n",
    "# 使用 re.sub() 移除这些匹配的文本\n",
    "#for key, value in enumerate(train_user['user_description']):\n",
    "#    print(key, re.sub(combined_pattern, '', re.sub(pattern, '', re.sub(r'<[^>]+>', '', train_user['user_description'][str(key)]))).replace('?','') if train_user['user_description'][str(key)] else 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_description = []\n",
    "# 使用 re.sub() 移除这些匹配的文本\n",
    "for key, value in enumerate(train_user['user_description']):\n",
    "    user_description.append(re.sub(combined_pattern, '', re.sub(pattern, '', re.sub(r'<[^>]+>', '', train_user['user_description'][str(key)]))).replace('?','').replace('*','') if train_user['user_description'][str(key)] else 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "print('read!')\n",
    "embeddings = embedder.encode(user_description)\n",
    "print(embeddings)\n",
    "print(len(embeddings[0]))\n",
    "#35m 28.0s for training data\n",
    "#28m 21.7s for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180581"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = np.vstack(embeddings)\n",
    "len(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "na = np.vstack(embeddings)\n",
    "np.save(\"/ssd5/ming/CPDN/dataset/features/test_user_description.npy\", na)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
